<details><summary>Lecture 1</summary>
   
<p>

[Power point](https://moodle.polytechnique.fr/pluginfile.php/482989/mod_resource/content/2/01-overview.pdf)
  
## Algorithms

An algorithm needs:
 1. A well-specified problem
 2. A method to solve it


An algorithm is *correct* if 
 1. it terminates
 2. it computes what its specification claims

Useful proof technique: look for **variants** and **invariants**:

```python
# Input:  x that can be multiplied
#         n nonnegative integer
# Output: x^n
def binpow(x,n):
  if n==0: return 1
  # n>0
  tmp = binpow(x,n//2) # n//2 < n
  # tmp = x^(n//2)
  tmp = tmp*tmp
  if n%2==0: return tmp
  return tmp*x
```

**Termination** is a very hard problem
- the general problem is **undecidable**

## Complexity

*How long will my program take?*
*Do I have enough money?*
  
The scientific approach:
  1. Experiment for various sizes;
  2. Model;
  3. Analyze the model;
  4. validate with experiments;
  5. If necessary, go to 2.
                                 
### Experimental Determination of (Polynomial) complexity

If the time for a computation grows like $C(n) \sim Kn^\alpha log^pn$ <br>
then doubling $n$ should take time $C(2n) \sim K2^\alpha n^\alpha log^pn$ <br>
so that $$\alpha \approx log_2 \left ( \dfrac{C(2n)}{C(n)} \right ) $$

### Notation

- $ f(n) \sim g(n) $ means $  \lim_{n\to\infty} f(n)/g(n) = 1  $
- $ f(n) = O(g(n)) $ means $ \exists K \exists M \forall n \geq M, |f(n)| \leq Kg(n) $
- $ f(n) = \Theta (g(n)) $ means $ f(n) = O(g(n)) $ and $ g(n) = O(f(n)) $

### Moore's "law"

The expression Moore's "law" is commonly used to mean that
> The speed and memory of computers is expected to **double every 18 months**.

![Graph of orders of Growth](CSE202_OrdersOfGrowth.png "Slide from the powerpoint")

## Lower Bounds

### Complexity of a problem

***Def.*** The *complexity of a problem* is that of the most efficient (possibly unknown) algorithm that solves it.

**Ex.** Sorting $n$ elements has complexity $O(nlogn)$ comparisons <br>
**Proof.** Mergesort (CSE103) reaches the bound

**Ex.** Sorting $n$ elements has complexity $\Theta (nlogn) $ comparisons. <br>
**Proof.** $k$ comparisons cannot distinguish more than $2^k$ permutations and $log_2(n!) \sim n log_2 (n)$

### Complexity of Powering

$$ (x,n) \in \mathbb{A} \times \mathbb{N} \mapsto x^n \in \mathbb{A} $$

We already know it is $O(logn)$ multiplications in $\mathbb{A}$ <br>
*Can this be improved?*

> **Lower bounds** on the complexity require a precise definition (a **model**) of what operations the "most efficient" algorithm can perform.

**Ex.** If the only available operation in $\mathbb{A}$ is multiplication, $x^{2^k}$ requires $k$ multiplications, so that $log_2n$ is a lower bound.

**Ex.** In floating point arithmetic, $x^n = exp(nlogx)$ and the complexity hardly depends on $n$.

### Simple lower bounds

In most useful models, reading the input and writing the output take time. Then, 
$$\text{size(Input) + size(Output) } \leq \text{complexity}$$

## Reductions

Problem X **reduces to** problem Y if you can use an algorithm that solves Y to help solve X.
$$\text{Complexity of solving X} = \text{complexity of solving Y} + \text{cost of the reduction}$$

- complexity of solving Y: perhaps many calls to Y on instances of different sizes (typically, one call)
- cost of the reduction: preprocessing and postprocessing (typically, less than the cost of solving Y)
</details>
